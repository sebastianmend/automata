import re

# Palabras reservadas del lenguaje nuevo
palabras_reservadas = {
    "let", "if", "then", "elseif", "else", "while", "do", "for", "in",
    "true", "false"
}

# Operadores
operadores = {
    "+=", "-=", "++", "==", "!=", "<=", ">=",
    "+", "-", "*", "/", "=", "<", ">"
}

# Delimitadores y s√≠mbolos
simbolos = {
    "(", ")", "{", "}", "[", "]", "<<", ">>", "[[", "]]",
    ",", ";", '"'
}

# Patrones regex para tokens
regex_token = {
    "numero_decimal": r"^-?\d+\.\d+$",
    "numero_entero": r"^-?\d+$",
    "cadena": r'^".*"$',
    "identificador": r"^[a-zA-Z_][a-zA-Z0-9_]*$"
}

def clasificar_token(token):
    if token in palabras_reservadas:
        return "Palabra reservada"
    elif token in operadores:
        return "Operador"
    elif token in simbolos:
        return "Delimitador"
    elif re.match(regex_token["numero_decimal"], token):
        return "N√∫mero decimal"
    elif re.match(regex_token["numero_entero"], token):
        return "N√∫mero entero"
    elif re.match(regex_token["cadena"], token):
        return "Cadena"
    elif re.match(regex_token["identificador"], token):
        return "Identificador"
    else:
        return "Error l√©xico"

def analizar_archivo():
    with open('ejemplo.txt', 'r', encoding='utf-8') as archivo:
        linea_n = 0
        total_tokens = 0
        errores = 0

        print("üîç ANALIZADOR L√âXICO - Lenguaje nuevo")
        print("=" * 50)

        for linea in archivo:
            linea_n += 1

            # Regex para extraer todos los tokens incluyendo comentarios
            tokens = re.findall(
                r'#.*|//.*|\'\'\'.*?\'\'\'|/\*.*?\*/|\+=|-=|\+\+|==|!=|<=|>=|<<|>>|\[\[|\]\]|"[^"]*"|[a-zA-Z_][a-zA-Z0-9_]*|-?\d+\.\d+|-?\d+|[^\s]',
                linea)

            if tokens:  # Solo mostrar l√≠neas con tokens
                print(f"\nL√≠nea {linea_n}: {linea.strip()}")

                for token in tokens:
                    total_tokens += 1

                    # Detectar comentarios
                    if (token.startswith('#') or token.startswith('//') or
                            token.startswith("'''") or token.startswith('/*')):
                        print(f"  ‚úÖ {token:<20} ‚ûú Comentario")
                    else:
                        tipo = clasificar_token(token)
                        if tipo != "Error l√©xico":
                            print(f"  ‚úÖ {token:<20} ‚ûú {tipo}")
                        else:
                            print(f"  ‚ùå {token:<20} ‚ûú Error l√©xico")
                            errores += 1

        # Resumen
        print("=" * 50)
        print(f"üìä Total tokens analizados: {total_tokens}")
        print(f"‚úÖ Tokens v√°lidos: {total_tokens - errores}")
        print(f"‚ùå Errores l√©xicos: {errores}")

        if errores == 0:
            print("üéâ ¬°An√°lisis completado sin errores!")


# Ejecutar an√°lisis
analizar_archivo()